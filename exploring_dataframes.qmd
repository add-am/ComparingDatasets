---
title: "Untitled"
format: html
---

# Introduction

Problem: Do all rows from a collection of spreadsheets exist in a master (AKA ground-truth) spreadsheet.

In a generalised sense, this problem can be visualised with the following diagrams.

 1. Imagine the master spreadsheet is represented by a grey rectangle
 2. Likewise, each of the other spreadsheets are represented by yellow, blue, and green rectangles

 []() Insert Image

 3. If all rows from the collection of spreadsheets (yellow, blue, green) exist in the master spreadsheet (grey), this can be visualised like this:

[]() Insert Image

 4. However if the collection of spreadsheets have columns that don't exist in the master, it would look like this:

[]() Insert Image

 5. Or if the collection had rows that didn't exist in the master it would look like this:

[]() Insert Image

The question becomes; do these rows/columns actually not exist in the master, or is there some small discrepency that is causing the issue?

```{r}

library(dplyr)
#library(readr)
library(readxl)
library(purrr)
library(stringr)

```

The first step is to read in all of the data. The master spreadsheet should be kept separate to the collection of spreadsheets.

```{r}

#get a list of datasets
file_names <- list.files("data/", full.names = TRUE)

#load the master (ground-truth) data by itself
gt_data <- read_xlsx(file_names[4], sheet = 2)

#load the collection of datasets together
collection <- map(file_names[1:3], ~read_xlsx(.x, 2))

```

Next we will need to compile a list of all column names that exist across both the gt and collection datasets. The idea here is to determine the amount of "true" variation in column names. To make things easier to check, the column names should be ordered alphabetically.

```{r}

#extract all column names
all_col_names <- sort(
    unlist(
        map(
            c(list(gt_data), collection), 
            colnames
        )
    )
)

```

```{r}

all_col_names

```

Looking at this list of names we can already spot some double ups, for instance "CHL (ug/L)" and "Chl_ug/L" refer to the same column and should have their name standardised. However, this is an area to tread cautiously - if you updated "XYZ (ug/L)" to match "XYZ_mg/L" you would be making a fatal error.

Generally it is a good idea to start with grammar and syntax, things like capitals, brackets, underscores, etc. Following this some obvious changes such as aligning "secchi depth m" and "secchi m" can occur.

Once you have standardised as much as reasonable, return only the unique names.

```{r}

all_col_names <- all_col_names |> 
    str_to_lower() |> 
    str_remove_all("\\(|\\)") |> 
    str_replace_all("_", " ") |> 
    str_replace_all("collection start date", "sample date") |> 
    str_replace_all("secchi depth m", "secchi m") |> 
    unique()

#generalise this method into a function for use later
col_name_cleaner <- function(input_string) {

    input_string |> 
        as.character() |> 
        str_to_lower() |> 
        str_remove_all("\\(|\\)") |> 
        str_replace_all("_", " ") |> 
        str_replace_all("collection start date", "sample date") |> 
        str_replace_all("secchi depth m", "secchi m")
}

```

This leaves us with a list of **all** possible column names across **all** of our datasets. Ideally we would then find that every single name in this list exists in the groundtruth spreadsheet. So lets check that out.

```{r}

#run function on gt data
gt_data <- gt_data |> 
    rename_with(
        .cols = everything(),
        ~col_name_cleaner(.)
    )

#compare gt data column names to the list of all names
exists <- all_col_names %in% colnames(gt_data)

```

Unfortunately what we actually find is that some names are not in the ground truth dataset, specifically the ones not present are:

```{r}

all_col_names[!exists]

```

Conversely, the colnames that don't exist in the collection are:

```{r}

collection <- map(collection, ~rename_with(.x, ~col_name_cleaner(.)))

exists_2 <- all_col_names %in% colnames(collection[[1]])

all_col_names[!exists_2]

```



Compile a "destination list" for all column names to be transformed into, experimenting to arrive at the lowest number of unique columns (whilst still retaining all required distinctions).

:::{.note}
This is a spot that will need to be customised for each new spreadsheet comparison as you explore the unique names, then the function below will need to be updated.
:::

```{r}

#duplicates
destination_names <- all_col_names

#make lower case, get unique, and sort
destination_names <- destination_names |> 
    str_to_lower() |> 
    unique() |> 
    sort()

#remove brackets, convert underscores to spaces, rename two columns, get unique
destination_names <- destination_names |> 
    str_remove_all("\\(|\\)") |> 
    str_replace_all("_", " ") |> 
    str_replace_all("collection start date", "sample date") |> 
    str_replace_all("secchi depth m", "secchi m") |> 
    unique()

```

Apply this transformation to each dataset via a generalised function. (BTW, it is easiest to treat the renaming process like you are dealing with a string rather than a df).

```{r}

col_name_cleaner <- function(input_string) {

    input_string |> 
        as.character() |> 
        str_to_lower() |> 
        str_remove_all("\\(|\\)") |> 
        str_replace_all("_", " ") |> 
        str_replace_all("collection start date", "sample date") |> 
        str_replace_all("secchi depth m", "secchi m")
}

clean_data <- map(data, ~rename_with(.x, ~col_name_cleaner(.)))

```

Filter data by "short name" to only keep sites of intest (this vector can be updated as needed).

```{r}

short_names_to_keep <- c(
    "BUR1", "BUR10", "BUR13", "BUR2", "BUR4", "BUR7", "BUR8", #(burdekin/dry tropics sites)
    "C1", "C11", "C4", "C5", "C6", "C8", #(Wet Tropics)
    "RM1", "RM10", "RM11", "RM12", "RM2", "RM3", "RM4", "RM5", "RM6", "RM7", "RM8", "RM9", #Wet Tropics)
    "TUL10", "TUL11", "TUL2", "TUL3", "TUL4", "TUL5", "TUL6", "TUL7", "TUL8", "TUL9",
    "WHI1", "WHI4", "WHI5", "WHI6", "WHI7" #(Whitsundays)
)

clean_filtered_data <- map(clean_data, ~filter(.x, `short name` %in% short_names_to_keep))


```

compare timeframes of each dataset

```{r}

time_frames <- map(clean_filtered_data, ~paste0(min(.x$`sample date`)," to ", max(.x$`sample date`)))

```

limit data to no later than the oldest sample from the "unknown" dataframe (see below for definition of "unknown").

```{r}

min_date <- min(clean_filtered_data[[1]]$`sample date`)

clean_filtered_data <- map(clean_filtered_data, ~filter(.x, `sample date` >= min_date))

```

Append a tracking column defining if the dataframe is "to be checked" or "doing the checking":

 - GT = "Ground Truth" (doing the checking)
 - U = "Unknown" (being checked) 

 ```{r}

#first make all df be unknown, then update just the one df to GT
clean_filtered_data <- map(clean_filtered_data, ~mutate(.x, type = "U", .before = 1))
clean_filtered_data[[4]] <- mutate(clean_filtered_data[[4]], type = "GT")

 ```

extract all of the datasets out of the list into one dataframe, then divide by the tracking column

```{r}

all_data <- do.call(bind_rows, clean_filtered_data)

unknown_data <- all_data |> filter(type == "U")

gt_data <- all_data |> filter(type == "GT")


```

Get a broad overview

```{r}

all.equal(unknown_data, gt_data)

 ```

 return rows that only exist in one table

 ```{r}

diff_1 <- anti_join(unknown_data, gt_data)

diff_2 <- anti_join(gt_data, unknown_data)

 ```

 ```{r}

install.packages("compare")

compare::compare(unknown_data, gt_data)

 ```