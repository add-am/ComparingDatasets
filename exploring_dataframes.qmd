---
title: "Untitled"
format: html
---

# Introduction

Problem: Do all rows from a collection of spreadsheets exist in a master (AKA ground-truth) spreadsheet.

In a generalised sense, this problem can be visualised with the following diagrams.

 1. Imagine the master spreadsheet is represented by a grey rectangle
 2. Likewise, each of the other spreadsheets are represented by yellow, blue, and green rectangles

 []() Insert Image

 3. If all rows from the collection of spreadsheets (yellow, blue, green) exist in the master spreadsheet (grey), this can be visualised like this:

[]() Insert Image

 4. However if the collection of spreadsheets have columns that don't exist in the master, it would look like this:

[]() Insert Image

 5. Or if the collection had rows that didn't exist in the master it would look like this:

[]() Insert Image

The question becomes; do these rows/columns actually not exist in the master, or is there some small discrepency that is causing the issue?

```{r}

library(dplyr)
#library(readr)
library(readxl)
library(purrr)
library(stringr)

```

The first step is to read in all of the data. The master spreadsheet should be kept separate to the collection of spreadsheets.

```{r}

#get a list of datasets
file_names <- list.files("data/", full.names = TRUE)

#load the master (ground-truth) data by itself
gt_data <- read_xlsx(file_names[4], sheet = 2)

#load the collection of datasets together
collection_data <- map(file_names[1:3], ~read_xlsx(.x, 2))

```

# Columns

Next we will need to compile a list of all column names that exist across both the gt and collection datasets. The idea here is to determine the amount of "true" variation in column names. To make things easier to check, the column names should be ordered alphabetically.

```{r}

#extract all column names
all_col_names <- sort(
    unlist(
        map(
            c(list(gt_data), collection_data), 
            colnames
        )
    )
)

```

```{r}

all_col_names

```

Looking at this list of names we can already spot some double ups, for instance "CHL (ug/L)" and "Chl_ug/L" refer to the same column and should have their name standardised. However, this is an area to tread cautiously - if you updated "XYZ (ug/L)" to match "XYZ_mg/L" you would be making a fatal error.

Generally it is a good idea to start with grammar and syntax, things like capitals, brackets, underscores, etc. Following this some obvious changes such as aligning "secchi depth m" and "secchi m" can occur. Finally, after some detailed exploration of the datasets we can indentify things such as "pn um" = "pn shim um", "tss mg/l" = "ss mg/l" and "depth avg to" = "acoustic depth".

```{r}

#create a list of replacements
replacements <- c(
    "collection start date" = "sample date",
    "secchi depth m" = "secchi m",
    "pn shim um" = "pn um",
    "^ss mg/l" = "tss mg/l",
    "depth avg to" = "acoustic depth"
)

all_col_names <- all_col_names |> 
    str_to_lower() |> 
    str_remove_all("\\(|\\)") |> 
    str_replace_all("_", " ") |> 
    str_replace_all(replacements) |> 
    unique()

#generalise this method into a function for use later
col_name_cleaner <- function(input_string) {

    input_string |> 
        as.character() |> 
        str_to_lower() |> 
        str_remove_all("\\(|\\)") |> 
        str_replace_all("_", " ") |> 
        str_replace_all(replacements)
}

```

This leaves us with a list of **all** possible unique column names across **all** of our datasets. Ideally we would then find that every single name in this list exists in the groundtruth spreadsheet. So lets check that out.

```{r}

#run function on gt data
gt_data <- gt_data |> 
    rename_with(
        .cols = everything(),
        ~col_name_cleaner(.)
    )

#compare gt data column names to the list of all names
all(all_col_names %in% colnames(gt_data))

```

Sucess!

Conversely, the colnames that don't exist in the collection are:

```{r}

collection_clean_data <- map(collection_data, ~rename_with(.x, ~col_name_cleaner(.)))

exists_2 <- all_col_names %in% colnames(collection_clean_data[[1]])

all_col_names[!exists_2]

```

On this side of things while more columns are missing, none are columns that we have historically used in the Technical Report. Therefore, for the purposes of this comparison these columns will be removed from the ground-truth dataset to basically arrive at the point where all columns are shared. Referencing the coloured rectangles we have arrive at this point:

[]() insert image

```{r}

gt_clean_data <- gt_data |> 
    select(!all_col_names[!exists_2])

```

# Rows

Now that columns are aligned, we can look at rows. Referencing the coloured rectangles again, our data looks something like this:

[]() insert image

In the case where we have more rows in our ground-truth dataset than the collections - this might be okay, the ground-truth dataset has more sites and a longer timeframe so we would expect this to answer that. However, in the case when rows exist in the collection that don't in the ground-truth... that is a problem. Lets take a look.

First we will filter data by "short name" to only keep sites of intest (this vector can be updated as needed).

```{r}

short_names_to_keep <- c(
    "BUR1", "BUR10", "BUR13", "BUR2", "BUR4", "BUR7", "BUR8", #(burdekin/dry tropics sites)
    "C1", "C11", "C4", "C5", "C6", "C8", #(Wet Tropics)
    "RM1", "RM10", "RM11", "RM12", "RM2", "RM3", "RM4", "RM5", "RM6", "RM7", "RM8", "RM9", #(Wet Tropics)
    "TUL10", "TUL11", "TUL2", "TUL3", "TUL4", "TUL5", "TUL6", "TUL7", "TUL8", "TUL9", #(Wet Tropics)
    "WHI1", "WHI4", "WHI5", "WHI6", "WHI7" #(Whitsundays)
)

#apply to ground_truth
gt_clean_filtered_data <- gt_clean_data |> 
    filter(`short name` %in% short_names_to_keep)

#apply to collection
collection_clean_filtered_data <- map(collection_clean_data, ~filter(.x, `short name` %in% short_names_to_keep))

```

Then we need to restrict our timeframes, looking at the min date from the collection is a reasonable cutoff point, but to ensure we don't make any mistakes we will move that to the start of the year rather than mid year (i.e. instead of 2020-07-09 we will use 2020-01-01).

```{r}

gt_clean_filtered_data <- gt_clean_filtered_data |> 
    filter(`sample date` >= "2020-01-01")

collection_clean_filtered_data <- map(collection_clean_filtered_data, ~filter(.x, `sample date` >= "2020-01-01"))

```

```{r}

length(unlist(map(collection_clean_filtered_data, ~unique(.x$`sample date`))))

```

```{r}

length(unique(gt_clean_filtered_data$`sample date`))

```

With both datasets capped to the same range, we can then append a tracking column to our two sets of data

 - GT = "Ground Truth" (doing the checking)
 - U = "Unknown" (being checked) 

 ```{r}

#first make all df be unknown, then update just the one df to GT
collection_clean_filtered_data <- map(collection_clean_filtered_data, ~mutate(.x, type = "U", .before = 1))
gt_clean_filtered_data <- mutate(gt_clean_filtered_data, type = "GT", .before = 1)

 ```

Following this, all data can be combined into one large dataset.

```{r}

all_data <- do.call(bind_rows, c(list(gt_clean_filtered_data), collection_clean_filtered_data))

```

And then we can group data by everything except the appended type column and count the number of rows in each group. The logic here is that each row in the collection, should have an equivalent row in the ground-truth dataset. Therefore, ever row with the type "U" should have a group count of 2. Conversely, most rows with the type "GT" should have a group count of 2, but it should be noted that the GT dataset has some more recent data, and some of the collection data doesn't have all samples for a site over the period. 

If a row with "U" has a group count of 1 - this is a big problem. If a row with "GT" has a group count of 1 - this is a small to moderate problem.

```{r}

all_data <- all_data |> 
    group_by(across(!type)) |> 
    mutate(count = n()) |> 
    ungroup()

```

```{r}

problem_rows <- all_data |> 
    filter(type == "U" & count < 2)

```

Our first assessment of these problems rows is that errors seem related to the time being missing in the date col (registering as 00:00:00), but there are still a few rows that do have times that have popped up. The errors for each of these have been identified as follows:

 - BUR4 2023-03-07 12:00:00 it seems like the GT table has no lat val
 - RM10 2023-01-26 13:00:00 the lat and long are .01 off
 - RM7 2023-01-26 12:10:00 long is .9 off - a big difference, only once though. a typo?
 - RM10 2023-01-12 12:44:00 the long is .01 off
 - RM10 2022-07-18 10:22:00 the long is .03 off
 - TUL 2023-01-11 10:55:00 the long is .12 off
 - BUR4 2023-01-04 11:05:00 the lat and long are .01 off

Overall, it looks like every issue of a row from a spreadsheet not found in the gt table is related to metadata.

Comparing in the other direction there are a few hundred rows with a group count of 1, so a manual check is probably not going to cut it.

```{r}

problem_rows_2 <- all_data |> 
    filter(type == "GT" & count < 2)

```
